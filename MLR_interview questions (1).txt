Normalization and Standardization are techniques used in data preprocessing to scale features so that they are comparable and improve the performance of machine learning models. Normalization typically rescales the data into a specific range, often [0,1], by applying the formula:(x−min)/(max−min).It is useful when features have different ranges, ensuring that no variable dominates due to its scale, especially in distance-based algorithms like KNN or clustering. Standardization, on the other hand, transforms the data so that it has a mean of 0 and a standard deviation of 1, using the formula:(x−μ)/σ.This is especially helpful when data is normally distributed and is widely used in algorithms like regression, SVM, and PCA. Both techniques make the learning process more stable and faster by reducing bias caused by varying feature scales.
Multicollinearity occurs when two or more independent variables in a regression model are highly correlated, leading to unreliable coefficient estimates and inflated standard errors. To address this issue, several techniques can be applied. One approach is to remove highly correlated predictors after analyzing the correlation matrix or Variance Inflation Factor (VIF). Another method is feature selection, where redundant features are dropped while retaining the most informative ones. Alternatively, dimensionality reduction techniques like Principal Component Analysis (PCA) can be used to combine correlated variables into uncorrelated components. Regularization methods, such as Ridge Regression (L2) and Lasso Regression (L1), are also effective since they shrink coefficients and reduce the impact of multicollinearity, with Lasso additionally performing variable selection. These approaches ensure a more stable and interpretable regression model.