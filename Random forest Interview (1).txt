Bagging (Bootstrap Aggregating) and Boosting are ensemble learning methods that combine multiple models to improve predictive performance. Bagging works by creating multiple subsets of data using bootstrapping (random sampling with replacement), training independent models (often decision trees) on each subset, and then aggregating their predictions by voting (for classification) or averaging (for regression). This reduces variance and helps avoid overfitting. Boosting, on the other hand, builds models sequentially, where each new model focuses on correcting the errors of the previous ones. It assigns higher weights to misclassified instances so that subsequent models pay more attention to difficult cases. Boosting reduces both bias and variance, often achieving higher accuracy, but it is more sensitive to noise. In summary, Bagging emphasizes parallel, independent learners to reduce variance, while Boosting emphasizes sequential, dependent learners to reduce bias and improve model performance.
Class imbalance occurs when one class has significantly more instances than the other(s), which can cause models to become biased toward the majority class. Several techniques are used to handle this issue. One common approach is resampling, which includes oversampling the minority class (e.g., SMOTE â€“ Synthetic Minority Oversampling Technique) or undersampling the majority class to balance the dataset. Another approach is to use cost-sensitive learning, where higher misclassification costs are assigned to the minority class so that the model penalizes mistakes more heavily. Additionally, ensemble methods like Balanced Random Forest or Boosting with class weights can improve performance on imbalanced data. Evaluation metrics such as Precision, Recall, F1-score, ROC-AUC, or PR-AUC should also be used instead of accuracy, as they give a more realistic picture of performance on imbalanced datasets.